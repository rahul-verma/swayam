{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating default router...\n"
     ]
    }
   ],
   "source": [
    "from test_utils import *\n",
    "from swayam import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling with Swayam Tools\n",
    "\n",
    "Let's now start to link the functionality of an LLM Agent with a locally available function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "router = Router(run_id=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see what happens without a function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Conversation with 1 step(s)\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrouter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPrompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the weather like in Boston?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/swayam/swayam/llm/router/__init__.py:102\u001b[0m, in \u001b[0;36mRouter.execute\u001b[0;34m(self, executable, reset_context, show_in_browser)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executable, UserPrompt):\n\u001b[1;32m    101\u001b[0m     log_debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting UserPrompt to LLMConversation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__execute_user_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executable, LLMConversation):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__execute_conversation(executable)\n",
      "File \u001b[0;32m~/Documents/GitHub/swayam/swayam/llm/router/__init__.py:47\u001b[0m, in \u001b[0;36mRouter.__execute_user_prompt\u001b[0;34m(self, user_prompt)\u001b[0m\n\u001b[1;32m     45\u001b[0m log_debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting UserPrompt to Conversation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m conversation \u001b[38;5;241m=\u001b[39m Conversation\u001b[38;5;241m.\u001b[39mfrom_prompts(user_prompt)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__execute_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/swayam/swayam/llm/router/__init__.py:72\u001b[0m, in \u001b[0;36mRouter.__execute_conversation\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs it is continued conversation, the system prompt is already set in the context. So, the context found in this conversation is going to be ignored. Review.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m log_debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting Conversation with ConversationAgent.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/swayam/swayam/llm/agent/conversation.py:55\u001b[0m, in \u001b[0;36mConversationAgent.execute\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmediator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mediator\n\u001b[1;32m     54\u001b[0m mediator \u001b[38;5;241m=\u001b[39m Mediator(model_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config, prompt_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_config, listener\u001b[38;5;241m=\u001b[39mlistener)\n\u001b[0;32m---> 55\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmediator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m log_debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Conversation\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[1;32m     57\u001b[0m listener\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/Documents/GitHub/swayam/swayam/llm/agent/mediator.py:84\u001b[0m, in \u001b[0;36mMediator.execute\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m     81\u001b[0m log_debug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished processing prompt...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m log_debug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting prompt...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m log_debug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHandling Response.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m output_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/GitHub/swayam/swayam/llm/model/__init__.py:83\u001b[0m, in \u001b[0;36mModel.create_client.<locals>.OpenAIModelClient.execute_messages\u001b[0;34m(self, messages, response_format, tools)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     77\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     78\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     79\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_kwargs\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_kwargs\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/lab/pyvenv/py3108/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py:118\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, seed, service_tier, stop, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m _validate_input_tools(tools)\n\u001b[1;32m    110\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Stainless-Helper-Method\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta.chat.completions.parse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    113\u001b[0m }\n\u001b[1;32m    115\u001b[0m raw_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    116\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    117\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m--> 118\u001b[0m     response_format\u001b[38;5;241m=\u001b[39m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    119\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    120\u001b[0m     function_call\u001b[38;5;241m=\u001b[39mfunction_call,\n\u001b[1;32m    121\u001b[0m     functions\u001b[38;5;241m=\u001b[39mfunctions,\n\u001b[1;32m    122\u001b[0m     logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[1;32m    123\u001b[0m     logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m    124\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m    125\u001b[0m     n\u001b[38;5;241m=\u001b[39mn,\n\u001b[1;32m    126\u001b[0m     parallel_tool_calls\u001b[38;5;241m=\u001b[39mparallel_tool_calls,\n\u001b[1;32m    127\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m    128\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    129\u001b[0m     service_tier\u001b[38;5;241m=\u001b[39mservice_tier,\n\u001b[1;32m    130\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    131\u001b[0m     stream_options\u001b[38;5;241m=\u001b[39mstream_options,\n\u001b[1;32m    132\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    133\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[1;32m    134\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    135\u001b[0m     top_logprobs\u001b[38;5;241m=\u001b[39mtop_logprobs,\n\u001b[1;32m    136\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    137\u001b[0m     user\u001b[38;5;241m=\u001b[39muser,\n\u001b[1;32m    138\u001b[0m     extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    139\u001b[0m     extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m    140\u001b[0m     extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m    141\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    144\u001b[0m     response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    145\u001b[0m     chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    146\u001b[0m     input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    147\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/lab/pyvenv/py3108/lib/python3.10/site-packages/openai/lib/_parsing/_completions.py:244\u001b[0m, in \u001b[0;36mtype_to_response_format_param\u001b[0;34m(response_format)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# type checkers don't narrow the negation of a `TypeGuard` as it isn't\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# a safe default behaviour but we know that at this point the `response_format`\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# can only be a `type`\u001b[39;00m\n\u001b[1;32m    242\u001b[0m response_format \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mtype\u001b[39m, response_format)\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_basemodel_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported response_format type - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     },\n\u001b[1;32m    254\u001b[0m }\n",
      "File \u001b[0;32m~/Documents/lab/pyvenv/py3108/lib/python3.10/site-packages/openai/lib/_parsing/_completions.py:220\u001b[0m, in \u001b[0;36mis_basemodel_type\u001b[0;34m(typ)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_basemodel_type\u001b[39m(typ: \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TypeGuard[\u001b[38;5;28mtype\u001b[39m[pydantic\u001b[38;5;241m.\u001b[39mBaseModel]]:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpydantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseModel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/abc.py:123\u001b[0m, in \u001b[0;36mABCMeta.__subclasscheck__\u001b[0;34m(cls, subclass)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__subclasscheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, subclass):\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override for issubclass(subclass, cls).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_abc_subclasscheck\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubclass\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "router.execute(Prompt.user_prompt(\"What's the weather like in Boston?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Dummy Functions\n",
    "\n",
    "Let's create some dummy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class UnitEnum(Enum):\n",
    "    CELSIUS = \"celsius\"\n",
    "    FAHRENHEIT = \"fahrenheit\"\n",
    "\n",
    "import json\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit):\n",
    "    import random\n",
    "    i = random.randint(-10, 40)\n",
    "    if unit.lower() == \"celsius\":\n",
    "        return f\"{i}°C\"\n",
    "    elif unit.lower() == \"fahrenheit\":\n",
    "        return f\"{i*9/5 + 32}°F\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid unit\")\n",
    "    \n",
    "def calculate(expression):\n",
    "    return eval(expression)\n",
    "\n",
    "def search_email(text):\n",
    "    import re\n",
    "    emails = re.findall(r'([\\w\\.-]+@[\\w\\.-]+)', text)\n",
    "    if emails:\n",
    "        return emails\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Creation\n",
    "\n",
    "As a part of an LLM workflow, we want to trigger the above local function based on response from the LLM (using function calling).\n",
    "\n",
    "Before we can do that, we need to create a sort of bridge between a function from the view point of an LLM vs what it means in Python.\n",
    "\n",
    "Here we see how we can create convert a function into a Tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToolBuilder\n",
    "\n",
    "We use the tool builder to wrap the function created above.\n",
    "1. Provide the function callable and describe it.\n",
    "2. Add all arguments, while describing their types and purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swayam import Tool\n",
    "builder = Tool.builder(get_current_weather, desc=\"Get the current weather in a given location\")\n",
    "builder.add_field(\"location\", type=str, desc=\"Location to get weather information for\")\n",
    "builder.add_field(\"unit\", type=UnitEnum, desc=\"Unit of temperature\", default=UnitEnum.CELSIUS)\n",
    "get_current_weather_tool = builder.build()\n",
    "\n",
    "builder = Tool.builder(calculate, desc=\"Evaluates a mathematical expression\")\n",
    "builder.add_field(\"expression\", type=str, desc=\"The expression to be evaluated.\")\n",
    "calc_tool = builder.build()\n",
    "\n",
    "builder = Tool.builder(search_email, desc=\"Searches email addresses in a given text\")\n",
    "builder.add_field(\"text\", type=str, desc=\"The target text in which emails are to be searched.\")\n",
    "search_email_tool = builder.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Execution\n",
    "\n",
    "The key thing to keep in mind is that all arguments are passed as keyword arguments irrespective of the underlying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_current_weather_tool(location=\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_current_weather_tool(**{\"location\": \"dummy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_current_weather_tool(**{\"location\": \"dummy\", \"unit\":\"fahrenheit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_tool(expression=\"2 + 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_email_tool(text=\"This is a sample text with a@b.com address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_email_tool(text=\"This is a sample text with no email address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Definition\n",
    "\n",
    "One of the key benefits of creating a Tool is that it self describes itself, in a way compatible with an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_current_weather_tool.definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_tool.definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search_email_tool.definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the list of tool definitions to be shared with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_weather_tool, calc_tool, search_email_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a tools dictionary so that once we know the name of the tool from LLM, we can call it with arguments returned from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_dict = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass tools to the user prompt with **tools** argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = Prompt.user_prompt(\"What's the weather like in Boston?\", tools=tools)\n",
    "response = router.execute(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for call in calls:\n",
    "    print(tools_dict[call.function.name](**json.loads(call.function.arguments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = agent.execute(\"What is 7 *30 + 13?\").tool_calls\n",
    "for call in calls:\n",
    "    print(tools_dict[call.function.name](**json.loads(call.function.arguments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample text with a@b.com and c@d.com addresses\"\n",
    "calls = agent.execute(f\"Find whether there is an email address in the following text marked with triple backticks \\n ```{text}```\").tool_calls\n",
    "for call in calls:\n",
    "    print(tools_dict[call.function.name](**json.loads(call.function.arguments)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3108",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
